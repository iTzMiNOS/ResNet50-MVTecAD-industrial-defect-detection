# -*- coding: utf-8 -*-
"""PyTorch_ResNet_Anomaly_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17FQRgz_7uLBKN0_tlnslhz_Tau8i7Ziu
"""

import urllib.request

urllib.request.urlretrieve("https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937484-1629951672/carpet.tar.xz", "carpet.tar.xz")

import tarfile

with tarfile.open("carpet.tar.xz") as f:
  f.extractall('.')

import matplotlib.pyplot as plt
from PIL import Image

image_path = "/content/carpet/test/hole/000.png"
image = Image.open(image_path)
image.size

from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

image.size

image_resized = transform(image)
image_resized.shape

plt.imshow(image_resized.permute(1, 2, 0))
plt.show()

memory_usage = image_resized.numel() * image_resized.element_size()

print(f"Memory usage of tensor: {memory_usage * 279//1024} KB")

from torchvision.datasets import ImageFolder

good_dataset = ImageFolder("/content/carpet/train", transform=transform)

x, y = good_dataset[0]

print("Image Shape: ",x.shape)
print("Label: ", y)

import torch

train_dataset, test_dataset = torch.utils.data.random_split(good_dataset, [0.8, 0.2])

print("Total number of samples in the original dataset: ", len(good_dataset))
print("Number of samples in the training dataset: ", len(train_dataset))
print("Number of samples in the test dataset: ", len(test_dataset))

from torch.utils.data import DataLoader

BS = 16

train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=True)

image_batch, label_batch = next(iter(train_loader))

print("Image Shape: ",image_batch.shape)
print("Label Shape: ", label_batch.shape)

memory_usage = image_batch.numel() * image_batch.element_size()

print(f"Memory usage of tensor: {memory_usage//1024} KB")

import torchvision

plt.figure(figsize=(12*4, 48*4))

grid = torchvision.utils.make_grid(image_batch[0:4], padding=5, nrow=4)

plt.imshow(grid.permute(1, 2, 0))
plt.title('Good Samples')
plt.show()







"""# **Auto Encoder**"""

import torch.nn.functional as F
from torch import nn

input_image = Image.open(r'/content/carpet/train/good/000.png')
input_image = transform(input_image)

plt.imshow(input_image.permute(1, 2, 0))
plt.show()

print("init: ", input_image.shape)

input_image = input_image.unsqueeze(0)
print("after unsqueeze: ", input_image.shape)

c1 = nn.Conv2d(in_channels=3, out_channels=128, kernel_size=4)
x = c1(input_image)
print("after conv: ", x.shape)

ap1 = nn.AvgPool2d(kernel_size=2, stride=2)
x = ap1(x)
print("after 1st avg pool: ", x.shape)

c2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4)
ap2 = nn.AvgPool2d(kernel_size=2, stride=2)
x = ap2(c2(x))
print("after 2nd conv&avg: ", x.shape)

c3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3)
ap3 = nn.AvgPool2d(kernel_size=2, stride=2)
x = ap3(c3(x))
print("after 3rd conv&avg: ", x.shape)

c4 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=4, stride=2, output_padding=1)
x = c4(x)
print("after 1st deconv: ", x.shape)

c5 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=5, stride=2, output_padding=1)
x = c5(x)
print("after 2nd deconv: ", x.shape)

c6 = nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=5, stride=2, output_padding=1)
x = c6(x)
print("after 3rd deconv: ", x.shape)

class AutoEncoder(nn.Module):
  def __init__(self):
    super(AutoEncoder, self).__init__()
    self.encoder = nn.Sequential(
        nn.Conv2d(3,128,kernel_size=4),
        nn.ReLU(),
        nn.AvgPool2d(kernel_size=2, stride=2),
        nn.Conv2d(128,256,kernel_size=4),
        nn.ReLU(),
        nn.AvgPool2d(kernel_size=2, stride=2),
        nn.Conv2d(256,256,kernel_size=3),
        nn.ReLU(),
        nn.AvgPool2d(kernel_size=2, stride=2)
    )
    self.decoder = nn.Sequential(
        nn.ConvTranspose2d(256,256,kernel_size=4, stride=2, output_padding=1),
        nn.ReLU(),
        nn.ConvTranspose2d(256,128,kernel_size=5, stride=2, output_padding=1),
        nn.ReLU(),
        nn.ConvTranspose2d(128,3,kernel_size=5, stride=2, output_padding=1),
        nn.Sigmoid()
    )

  def forward(self, x):
    x = self.encoder(x)
    x = self.decoder(x)
    return x

model = AutoEncoder()
input_image = torch.randn(1, 3, 224, 224)
output_image = model(input_image)
print(output_image.shape)

model.cuda()
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

Loss = []
Validation_Loss = []

from tqdm import tqdm
EPOCHS = 100
for epoch in tqdm(range(EPOCHS)):
  model.train()
  for img, _ in train_loader:
    img = img.cuda()
    output = model(img)
    loss = criterion(output, img)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  Loss.append(loss.item())


  model.eval()
  with torch.no_grad():
    val_loss_sum = 0.0
    num_batches = 0
    for img, _ in test_loader:
      img = img.cuda()
      output = model(img)
      val_loss = criterion(output, img)
      val_loss_sum += val_loss.item()
      num_batches += 1
    val_loss_avg = val_loss_sum / num_batches
    Validation_Loss.append(val_loss_avg)

  if epoch % 5 == 0:
    print(f"Epoch: {epoch} / {EPOCHS}, Loss: {loss.item()}, Validation Loss: {val_loss_avg}")

plt.plot(Loss, label='Train Loss')
plt.plot(Validation_Loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

torch.save(model.state_dict(), 'autoencoder_v1.pth')
model.eval()

ckpoints = torch.load('autoencoder_v1.pth')
# model = AutoEncoder() need to run if loading from other env
model.load_state_dict(ckpoints)







"""# **Reconstruction**"""

with torch.no_grad():
  for data, _ in train_loader:
    data = data.cuda()
    recon = model(data)
    break

recon_error = ((data - recon)**2).mean(axis=1)
print(recon_error.shape)

plt.imshow(recon_error[0][0:-10, 0:-10].cpu().numpy())

plt.figure(dpi=250)
fig, ax = plt.subplots(3, 3, figsize=(5*4, 4*4))
for i in range(3):
  ax[0, i].imshow(data[i].cpu().numpy().transpose((1, 2, 0)))
  ax[1, i].imshow(recon[i].cpu().numpy().transpose((1, 2, 0)))
  ax[2, i].imshow(recon_error[i][0:-10, 0:-10].cpu().numpy(), cmap='jet', vmax=torch.max(recon_error[i]))
  ax[0, i].axis('OFF')
  ax[1, i].axis('OFF')
  ax[2, i].axis('OFF')
plt.show()

data_list = [None]*3
data_list[0] = torch.stack([transform(Image.open(f'/content/carpet/test/color/00{i}.png')) for i in range(3)])
data_list[1] = torch.stack([transform(Image.open(f'/content/carpet/test/cut/00{i}.png')) for i in range(3)])
data_list[2] = torch.stack([transform(Image.open(f'/content/carpet/test/hole/00{i}.png')) for i in range(3)])

with torch.no_grad():
  data = data_list[0].cuda()
  recon = model(data)

recon_error = ((data - recon)**2).mean(axis=1)

plt.figure(dpi=250)
fig, ax = plt.subplots(3, 3, figsize=(5*4, 4*4))
for i in range(3):
  ax[0, i].imshow(data[i].cpu().numpy().transpose((1, 2, 0)))
  ax[1, i].imshow(recon[i].cpu().numpy().transpose((1, 2, 0)))
  ax[2, i].imshow(recon_error[i][0:-10, 0:-10].cpu().numpy(), cmap='jet', vmax=torch.max(recon_error[i]))
  ax[0, i].axis('OFF')
  ax[1, i].axis('OFF')
  ax[2, i].axis('OFF')
plt.show()

with torch.no_grad():
  data = data_list[1].cuda()
  recon = model(data)

recon_error = ((data - recon)**2).mean(axis=1)

plt.figure(dpi=250)
fig, ax = plt.subplots(3, 3, figsize=(5*4, 4*4))
for i in range(3):
  ax[0, i].imshow(data[i].cpu().numpy().transpose((1, 2, 0)))
  ax[1, i].imshow(recon[i].cpu().numpy().transpose((1, 2, 0)))
  ax[2, i].imshow(recon_error[i][0:-10, 0:-10].cpu().numpy(), cmap='jet', vmax=torch.max(recon_error[i]))
  ax[0, i].axis('OFF')
  ax[1, i].axis('OFF')
  ax[2, i].axis('OFF')
plt.show()

with torch.no_grad():
  data = data_list[2].cuda()
  recon = model(data)

recon_error = ((data - recon)**2).mean(axis=1)

plt.figure(dpi=250)
fig, ax = plt.subplots(3, 3, figsize=(5*4, 4*4))
for i in range(3):
  ax[0, i].imshow(data[i].cpu().numpy().transpose((1, 2, 0)))
  ax[1, i].imshow(recon[i].cpu().numpy().transpose((1, 2, 0)))
  ax[2, i].imshow(recon_error[i][0:-10, 0:-10].cpu().numpy(), cmap='jet', vmax=torch.max(recon_error[i]))
  ax[0, i].axis('OFF')
  ax[1, i].axis('OFF')
  ax[2, i].axis('OFF')
plt.show()

recon_error_values = []
with torch.no_grad():
  for data, _ in train_loader:
    data = data.cuda()
    recon = model(data)
    data_recon_MSE = ((data - recon)**2).mean(axis=(1))[:, 0:-10, 0:-10].mean(axis=(1, 2))
    recon_error_values.append(data_recon_MSE)

recon_error_values = torch.cat(recon_error_values).cpu().numpy()

import numpy as np
opt_thresh = np.mean(recon_error_values) + 3 * np.std(recon_error_values)

plt.hist(recon_error_values, bins=50)
plt.vlines(x=opt_thresh, color='r', ymin=0, ymax=30)
plt.show()

y_true = []
y_pred = []
y_score = []

model.eval()

from pathlib import Path


with torch.no_grad():

  test = Path('/content/carpet/test')
  for path in test.glob('*/*.png'):
    fault_type = path.parts[-2]
    test_image = transform(Image.open(path)).cuda().unsqueeze(0)
    recon_image = model(test_image)

    y_score_image = ((test_image - recon_image)**2).mean(axis=(1))[:,0:-10, 0:-10].mean()

    y_pred_image = 1*(y_score_image >= opt_thresh)

    y_true_image = 0 if fault_type == 'good' else 1

    y_true.append(y_true_image)
    y_pred.append(y_pred_image.cpu())
    y_score.append(y_score_image.cpu())

y_true = np.array(y_true)
y_pred = np.array(y_pred)
y_score = np.array(y_score)

plt.hist(y_score, bins=50)
plt.vlines(x=opt_thresh, color='r', ymin=0, ymax=30)
plt.show()

from sklearn.metrics import roc_auc_score, roc_curve
import seaborn as sns

auc_roc_score = roc_auc_score(y_true, y_score)
print("AUC-ROC Score: ", auc_roc_score)

fpr, tpr, thresholds = roc_curve(y_true, y_score)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc_score)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()











"""# **ResNet50**"""

from torchvision.models import resnet50, ResNet50_Weights

transform = transforms.Compose([
  transforms.Resize((224, 224)),
  transforms.ToTensor(),
])

train_image_path = Path('/content/carpet/train')

good_dataset = ImageFolder(train_image_path, transform=transform)
train_dataset, test_dataset = torch.utils.data.random_split(good_dataset, [0.8, 0.2])

BS = 16

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BS, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BS, shuffle=False)

resnet_model = resnet50(weights=ResNet50_Weights.DEFAULT)

for layer in resnet_model.named_parameters():
  print(layer[0])

#layer 2 output layer2.3.bn3.bias
#layer 3 output layer3.5.bn3.bias

def hook(model, input, output):
  feature.append(output.detach())

feature = []

resnet_model.layer2[-1].register_forward_hook(hook)
resnet_model.layer3[-1].register_forward_hook(hook)

_ = resnet_model(torch.randn(1,3,224,224))

feature[0].shape

feature[1].shape

len(feature)

resnet_model.layer2[-1]

resnet_model.layer3[-1]



"""# **ResNet Feature Extractor Class**"""

class resnet_feature_extractor(nn.Module):
  def __init__(self):
    super(resnet_feature_extractor, self).__init__()
    self.model = resnet50(weights=ResNet50_Weights.DEFAULT)

    self.model.eval()
    for param in self.model.parameters():
      param.requires_grad = False

    def hook(model, input, output):
      self.features.append(output)

    self.model.layer2[-1].register_forward_hook(hook)
    self.model.layer3[-1].register_forward_hook(hook)

  def forward(self, input):
    self.features = []
    with torch.no_grad():
      _ = self.model(input)

    self.avg = torch.nn.AvgPool2d(3, stride=1)
    fmap_size = self.features[0].shape[-2]
    self.resize = torch.nn.AdaptiveAvgPool2d(fmap_size)

    resized_maps = [self.resize(self.avg(fmap)) for fmap in self.features]
    patch = torch.cat(resized_maps, 1)

    return patch

image = Image.open('/content/carpet/test/color/000.png')
image = transform(image)
image = image.unsqueeze(0)
backbone = resnet_feature_extractor()
feature = backbone(image)

print(backbone.features[0].shape)
print(backbone.features[1].shape)
print(feature.shape)

plt.imshow(image[0].permute(1,2,0))

indices = torch.randperm(64)[:10]

fig, axes = plt.subplots(2, 5, figsize=(15, 6))
for i, index in enumerate(indices):
  row = i // 5
  col = i % 5
  axes[row, col].imshow(feature[0, index].detach().cpu(), cmap='gray')
  axes[row, col].set_title(f'Feature Map {index}')
  axes[row, col].axis('off')
plt.tight_layout()
plt.show()



"""# **ResNet AutoEncoder**"""

import torch.nn as nn

class FeatCAE(nn.Module):
    def __init__(self, in_channels=1000, latent_dim=50, is_bn=True):
        super(FeatCAE, self).__init__()

        layers = []
        layers += [nn.Conv2d(in_channels, (in_channels + 2 * latent_dim) // 2, kernel_size=1, stride=1, padding=0)]
        if is_bn:
            layers += [nn.BatchNorm2d(num_features=(in_channels + 2 * latent_dim) // 2)]
        layers += [nn.ReLU()]
        layers += [nn.Conv2d((in_channels + 2 * latent_dim) // 2, 2 * latent_dim, kernel_size=1, stride=1, padding=0)]
        if is_bn:
            layers += [nn.BatchNorm2d(num_features=2 * latent_dim)]
        layers += [nn.ReLU()]
        layers += [nn.Conv2d(2 * latent_dim, latent_dim, kernel_size=1, stride=1, padding=0)]

        self.encoder = nn.Sequential(*layers)

        layers = []
        layers += [nn.Conv2d(latent_dim, 2 * latent_dim, kernel_size=1, stride=1, padding=0)]
        if is_bn:
            layers += [nn.BatchNorm2d(num_features=2 * latent_dim)]
        layers += [nn.ReLU()]
        layers += [nn.Conv2d(2 * latent_dim, (in_channels + 2 * latent_dim) // 2, kernel_size=1, stride=1, padding=0)]
        if is_bn:
            layers += [nn.BatchNorm2d(num_features=(in_channels + 2 * latent_dim) // 2)]
        layers += [nn.ReLU()]
        layers += [nn.Conv2d((in_channels + 2 * latent_dim) // 2, in_channels, kernel_size=1, stride=1, padding=0)]

        self.decoder = nn.Sequential(*layers)

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

import torch.optim as optim

model = FeatCAE(in_channels=1536, latent_dim=100).cuda()
backbone.cuda()

criterion = torch.nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

Loss = []
Validation_Loss = []


EPOCHS = 50
for epoch in tqdm(range(EPOCHS)):
    model.train()
    for data,_ in train_loader:
        with torch.no_grad():
            features = backbone(data.cuda())
        output = model(features)
        loss = criterion(output, features)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    Loss.append(loss.item())

    model.eval()
    with torch.no_grad():
        val_loss_sum = 0.0
        num_batches = 0
        for data, _ in test_loader:
            features = backbone(data.cuda())
            output = model(features)
            val_loss = criterion(output, features)
            val_loss_sum += val_loss.item()
            num_batches += 1
        val_loss_avg = val_loss_sum / num_batches
        Validation_Loss.append(val_loss_avg)

    if epoch % 5 == 0:
      print(f"Epoch: {epoch} / {EPOCHS}, Loss: {loss.item()}, Validation Loss: {val_loss_avg}")

plt.plot(Loss, label='Training Loss')
plt.plot(Validation_Loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

torch.save(model.state_dict(), 'autoencoder_resnet.pth')
model.eval()

ckpoints = torch.load('autoencoder_resnet.pth')
# model = AutoEncoder() need to run if loading from other env
model.load_state_dict(ckpoints)

model.eval()

image = Image.open('/content/carpet/test/hole/002.png')
image = transform(image)
image = image.unsqueeze(0)

with torch.no_grad():
  features = backbone(image.cuda())
  recon = model(features)

recon_error = ((features - recon)**2).mean(axis=(1)).unsqueeze(0)

segm_map = torch.nn.functional.interpolate(
    recon_error,
    size=(224, 224),
    mode='bilinear',
)

plt.imshow(segm_map.squeeze().cpu().numpy(), cmap='jet')
plt.show()

def decision_function(segm_map):
  mean_top_10_values = []

  for map in segm_map:
    flattened_tensor = map.reshape(-1)

    sorted_tensor, _ = torch.sort(flattened_tensor, descending=True)

    top_10_values = sorted_tensor[:10]

    mean_top_10_value = sorted_tensor[:10].mean()

    mean_top_10_values.append(mean_top_10_value)

  return torch.stack(mean_top_10_values)

model.eval()

recon_error_values = []
for data, _ in train_loader:
  with torch.no_grad():
    features = backbone(data.cuda()).squeeze()
    recon = model(features)
  segm_map = ((features - recon)**2).mean(axis=(1))[:,3:-3,3:-3]
  anomaly_score = decision_function(segm_map)

  recon_error_values.append(anomaly_score)

recon_error_values = torch.cat(recon_error_values).cpu().numpy()

opt_thresh = np.mean(recon_error_values) + 3 * np.std(recon_error_values)

heat_map_max, heat_map_min = np.max(recon_error_values), np.min(recon_error_values)

plt.hist(recon_error_values, bins=50)
plt.vlines(x=opt_thresh, color='r', ymin=0, ymax=30)
plt.show()

y_true = []
y_pred = []
y_score = []

model.eval()
backbone.eval()

with torch.no_grad():

  test = Path('/content/carpet/test')
  for path in test.glob('*/*.png'):
    fault_type = path.parts[-2]
    test_image = transform(Image.open(path)).cuda().unsqueeze(0)

    with torch.no_grad():
      features = backbone(test_image)
      recon = model(features)

    segm_map = ((features - recon)**2).mean(axis=(1))[:,3:-3, 3:-3]

    y_score_image = decision_function(segm_map)

    y_pred_image = 1*(y_score_image >= opt_thresh)

    y_true_image = 0 if fault_type == 'good' else 1

    y_true.append(y_true_image)
    y_pred.append(y_pred_image.cpu())
    y_score.append(y_score_image.cpu())

y_true = np.array(y_true)
y_pred = np.array(y_pred)
y_score = np.array(y_score)

plt.hist(y_score, bins=50)
plt.vlines(x=opt_thresh, color='r', ymin=0, ymax=30)
plt.show()

from sklearn.metrics import roc_auc_score, roc_curve
import seaborn as sns

auc_roc_score = roc_auc_score(y_true, y_score)
print("AUC-ROC Score: ", auc_roc_score)

fpr, tpr, thresholds = roc_curve(y_true, y_score)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc_score)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score

f1_scores = [f1_score(y_true, y_score >= threshold) for threshold in thresholds]

opt_thresh = thresholds[np.argmax(f1_scores)]
print("Optimal Threshold: ", opt_thresh)

cm = confusion_matrix(y_true, (y_score >= opt_thresh).astype(int))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['good', 'faulty'])
disp.plot()
plt.show()

import cv2
import torch
import matplotlib.pyplot as plt
from PIL import Image
from IPython.display import clear_output
from pathlib import Path

model.eval()
backbone.eval()

test_path = Path('/content/carpet/test')

shown_types = set()

fault_types = ["color", "cut", "good", "hole", "metal_contamination", "thread"]
class_label = ['good', 'faulty']

for path in test_path.glob('*/*.png'):
    fault_type = path.parts[-2]

    if fault_type in shown_types:
        continue

    test_image = transform(Image.open(path)).cuda().unsqueeze(0)

    with torch.no_grad():
        features = backbone(test_image)
        recon = model(features)

    segm_map = ((features - recon)**2).mean(axis=(1))
    y_score_image = decision_function(segm_map=segm_map)
    y_pred_image = 1 * (y_score_image >= opt_thresh)

    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.imshow(test_image.squeeze().permute(1, 2, 0).cpu().numpy())
    plt.title(f'fault type: {fault_type}')

    plt.subplot(1, 3, 2)
    heat_map = segm_map.squeeze().cpu().numpy()
    heat_map = cv2.resize(heat_map, (128, 128))
    plt.imshow(heat_map, cmap='jet', vmin=heat_map_min, vmax=heat_map_max * 10)
    plt.title(f'Anomaly score: {y_score_image[0].cpu().numpy() / opt_thresh:0.4f} || {class_label[y_pred_image]}')

    plt.subplot(1, 3, 3)
    plt.imshow((heat_map > opt_thresh * 10), cmap='gray')
    plt.title('Segmentation Map')

    plt.show()

    shown_types.add(fault_type)

    if len(shown_types) == len(fault_types):
        break

